# ComfyUI_Float_Animator

A custom node package for ComfyUI that integrates the powerful [FLOAT](https://github.com/deepbrainai-research/float) project, enabling audio-driven talking portrait video generation directly within your ComfyUI workflow.

<div align="center">
  <video src="https://github.com/user-attachments/assets/36626b4a-d3e5-4db9-87a7-ca0e949daee0" /> <!-- Reference to an example video -->
</div>

---
---

## Nodes

*   **Float_Animator**: Generates speaking portrait video frames from an image and audio.

## Node Descriptions

### 1. Float_Animator

*   **Function**: Takes a still portrait image and an audio file to generate a sequence of animated frames, synchronizing lip movements and allowing emotion control based on the FLOAT model.
*   **Key Inputs**:
    *   `ref_image`: (IMAGE) The still portrait image to animate.
    *   `audio`: (AUDIO) The driving audio to synchronize with the portrait.
    *   `seed`: (INT) Random seed for reproducibility of results, with "control after generate" options.
    *   `emotion`: (ENUM) Specify a target emotion style. Choose 'none' to infer emotion from the driving audio.
    *   `fps`: (FLOAT) Frames per second for the output animation.
    *   `aud_cfg_scale`: (FLOAT) Classifier-free guidance scale for audio control (default: 2.0).
    *   `ref_cfg_scale`: (FLOAT) Classifier-free guidance scale for reference image control (default: 1.0).
    *   `emo_cfg_scale`: (FLOAT) Classifier-free guidance scale for emotion control (default: 1.0).
    *   `model`: (COMBO) Select the FLOAT main model file (.pth) to use from `ComfyUI/models/Float/`.
    *   `auto_crop`: (BOOLEAN) Automatically crop the face in the reference image. (Default: Off, enables black padding if needed)
*   **Outputs**:
    *   `animated_frames`: (IMAGE) The generated sequence of animated image frames (ComfyUI standard `IMAGE` format for video output).
    *   `audio`: (AUDIO) The input audio object passed through.
    *   `fps`: (FLOAT) The frames per second used for the output animation.
*   **Usage**: Connect a `Load Image` node to `ref_image` and a `Load Audio` node to `audio`. Select your desired FLOAT model from the `model` dropdown. Adjust `fps`, guidance scales (`aud_cfg_scale`, `ref_cfg_scale`, `emo_cfg_scale`), `emotion`, and `auto_crop` as needed. Set a `seed` for reproducible results. The output `animated_frames` can be connected to `Save Image` for an image sequence or a `Video Combine` node (e.g., `VHS_VideoCombine`) to create a video. The `audio` output can be passed to audio-related nodes, and `fps` output can be used to drive video combining nodes.

![image](https://github.com/KERRY-YUAN/ComfyUI_Float_Animator/blob/main/Examples/ComfyUI_Float_Animator.png)
![image](https://github.com/KERRY-YUAN/ComfyUI_Float_Animator/blob/main/Examples/Spark_TTS_&_Float_Animator.png)
---
---

## Installation Steps

1.  **Navigate to ComfyUI `custom_nodes` directory:**
    ```bash
    cd path/to/your/ComfyUI/custom_nodes
    ```
2.  **Clone this repository:**
    ```bash
    git clone https://github.com/KERRY-YUAN/ComfyUI_Float_Animator.git ComfyUI_Float_Animator
    cd ComfyUI_Float_Animator
    ```
3.  **Install Dependencies:**
    Install the required Python libraries using your ComfyUI Python environment.
    ```bash
    # Example for ComfyUI's embedded Python on Windows:
    # path/to/your/ComfyUI/python_embeded/python.exe -m pip install -r requirements.txt
    # 
    # Or for a system-wide/venv Python:
    pip install -r requirements.txt
    ```
    *Note: Ensure `torch` and `torchaudio` versions are compatible with your system and ComfyUI's existing PyTorch installation.*

## ğŸ“¥ Model and Data Setup

The FLOAT model and its associated data **must be placed in specific default locations** for the node to function correctly. **This node package now supports automatic model download when the node is executed for the first time or if models are missing.** You can also use the `Model_Download.bat` script provided with this node package to download and place them automatically beforehand.


1.  **Model Location:**
    The `float.pth` main model, `wav2vec2-base-960h` folder, and `wav2vec-english-speech-emotion-recognition` folder must be located at: `ComfyUI/models/Float/`
	`float.pth`: https://drive.google.com/file/d/1rvWuM12cyvNvBQNCLmG4Fr2L1rpjQBF0/view?usp=sharing
	`wav2vec2-base-960h`: https://huggingface.co/facebook/wav2vec2-base-960h
	`wav2vec-english-speech-emotion-recognition`: https://huggingface.co/r-f/wav2vec-english-speech-emotion-recognition

2.  **Automatic Download (Node Execution):**
    Simply open the `Float_Animator` node in ComfyUI. If the required models are not found, the node will initiate their download automatically.

3.  **Manual Download (Optional / Troubleshooting):**
    If the automatic download within ComfyUI fails, or you prefer to pre-download models, navigate to the `ComfyUI_Float_Animator` directory and run the provided batch script:
    ```bash
    cd ComfyUI/custom_nodes/ComfyUI_Float_Animator
    .\Model_Download.bat # Run this script on Windows
    ```
    (For Linux/macOS users, refer to the `Model_Download.bat` content or the original FLOAT repository for manual download commands, or run `python model_download/model_download.py` directly).

4.  **Directory Structure Reference:**
    The required final file structure is:

    ```
    ComfyUI/
    â”œâ”€â”€ custom_nodes/
    â”‚   â””â”€â”€ ComfyUI_Float_Animator/
    â”‚       â”œâ”€â”€ models/             # FLOAT's internal model definitions
    â”‚       â”‚   â”œâ”€â”€ float/          
    â”‚       â”‚   â”‚   â”œâ”€â”€ encoder.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ FLOAT.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ FMT.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ generator.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ styledecoder.py
    â”‚       â”‚   â”‚   â””â”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ wav2vec2.py
    â”‚       â”‚   â”œâ”€â”€ wav2vec2_ser.py
    â”‚       â”‚   â””â”€â”€ __init__.py
    â”‚       â”œâ”€â”€ model_download/     # Model download scripts and configuration
    â”‚       â”‚   â”œâ”€â”€ model_download.py
    â”‚       â”‚   â”œâ”€â”€ model_list.json # Model list in JSON format
    â”‚       â”‚   â””â”€â”€ __init__.py
    â”‚       â”œâ”€â”€ options/
    â”‚       â”œâ”€â”€ Node.py
    â”‚       â”œâ”€â”€ Model_Download.bat  # Manual download script
    â”‚       â”œâ”€â”€ requirements.txt
    â”‚       â””â”€â”€ ... (other package files)
    â””â”€â”€ models/
        â””â”€â”€ Float/
            â”œâ”€â”€ float.pth                           # Main FLOAT model checkpoint
            â”œâ”€â”€ wav2vec2-base-960h/                 # Audio encoder model folder
            â”‚   â””â”€â”€ ... (files from Hugging Face)
            â””â”€â”€ wav2vec-english-speech-emotion-recognition/  # Emotion encoder model folder
                â””â”€â”€ ... (files from Hugging Face)
    ```


## ğŸ’¡ Optimization Tips for Synthesis Quality

*   **Frontal Head Pose**: The FLOAT model performs best on images with a frontal head pose. Non-frontal images may lead to suboptimal results.
*   **Face Cropping**: The `auto_crop` option is now *off* by default. If your reference image contains a full head shot that might benefit from automatic cropping and scaling (which may introduce black padding regions around the face), you should enable this option. The original FLOAT project recommends cropping for optimal results.
*   **Audio Quality**: For best lip-sync and emotion generation, use clean audio with minimal background music. Tools like [ClearVoice](https://github.com/modelscope/ClearerVoice-Studio) can help in extracting vocals.

## â— License â—

This ComfyUI node wrapper's code is released under the MIT License (see `LICENSE.md` in this repository).

**However, the underlying FLOAT model and its core inference code (including files in the `models/` and `options/` directories, and parts adapted from the official `generate.py`) are released under the [Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License (CC BY-NC-ND 4.0)](https://creativecommons.org/licenses/by-nc-nd/4.0/).**

**This means the underlying FLOAT technology cannot be used for commercial purposes and no adaptations are permitted (beyond necessary technical integration for framework compatibility).** è¯·æŸ¥é˜… `LICENSE.md` æ–‡ä»¶äº†è§£å®Œæ•´è¯¦æƒ…ã€‚

For commercial inquiries regarding the original FLOAT project, please contact `daniel@deepbrain.io` as per their official repository.

## ğŸ™ Acknowledgements

This project is a ComfyUI wrapper around the excellent work by Taekyung Ki, Dongchan Min, and Gyeongsu:
```bibtex
@article{ki2024float,
  title={FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait},
  author={Ki, Taekyung and Min, Dongchan and Chae, Gyeongsu},
  journal={arXiv preprint arXiv:2412.01064},
  year={2024}
}
```
We are grateful to the original authors for their valuable contributions and for open-sourcing their code. We thank the developers of the original [FLOAT](https://github.com/deepbrainai-research/float) project for the powerful model and library components used in this node.

---
---

# ComfyUI_Float_Animator (ä¸­æ–‡)

ä¸€ä¸ªç”¨äº ComfyUI çš„è‡ªå®šä¹‰èŠ‚ç‚¹åŒ…ï¼Œé›†æˆäº†å¼ºå¤§çš„ [FLOAT](https://github.com/deepbrainai-research/float) é¡¹ç›®ï¼Œå¯ä»¥ç›´æ¥åœ¨ ComfyUI å·¥ä½œæµä¸­å®ç°éŸ³é¢‘é©±åŠ¨çš„è¯´è¯è‚–åƒè§†é¢‘ç”Ÿæˆã€‚

<div align="center">
  <video src="https://github.com/user-attachments/assets/36626b4a-d3e5-4db9-87a7-ca0e949daee0" /> <!-- å¼•ç”¨ç¤ºä¾‹è§†é¢‘ -->
</div>

---
---

## èŠ‚ç‚¹åˆ—è¡¨

*   **Float_Animator**: ç”±è‚–åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„è¯´è¯è§†é¢‘ã€‚

## èŠ‚ç‚¹è¯´æ˜

### 1. Float_Animator

*   **åŠŸèƒ½**: æ¥æ”¶ä¸€å¼ é™æ€è‚–åƒå›¾åƒå’Œä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ŒåŸºäº FLOAT æ¨¡å‹ç”Ÿæˆä¸€ç³»åˆ—åŠ¨ç”»å¸§ï¼ŒåŒæ­¥å”‡éƒ¨åŠ¨ä½œå¹¶æ”¯æŒæƒ…æ„Ÿæ§åˆ¶ã€‚
*   **ä¸»è¦è¾“å…¥**:
    *   `ref_image`: (å›¾åƒ) ç”¨äºåŠ¨ç”»çš„é™æ€è‚–åƒå›¾åƒã€‚
    *   `audio`: (éŸ³é¢‘) é©±åŠ¨è‚–åƒåŠ¨ç”»çš„éŸ³é¢‘ã€‚
    *   `seed`: (æ•´æ•°) ç”¨äºç»“æœå¤ç°çš„éšæœºç§å­ï¼Œå¸¦æœ‰â€œç”Ÿæˆåæ§åˆ¶â€é€‰é¡¹ã€‚
    *   `emotion`: (æšä¸¾) æŒ‡å®šç›®æ ‡æƒ…æ„Ÿé£æ ¼ã€‚é€‰æ‹© 'none' å°†ä»é©±åŠ¨éŸ³é¢‘ä¸­æ¨æ–­æƒ…æ„Ÿã€‚
    *   `fps`: (æµ®ç‚¹æ•°) è¾“å‡ºåŠ¨ç”»çš„æ¯ç§’å¸§æ•°ã€‚
    *   `aud_cfg_scale`: (æµ®ç‚¹æ•°) éŸ³é¢‘æ§åˆ¶çš„æ— åˆ†ç±»å™¨å¼•å¯¼å°ºåº¦ï¼ˆé»˜è®¤å€¼ï¼š2.0ï¼‰ã€‚
    *   `ref_cfg_scale`: (æµ®ç‚¹æ•°) å‚è€ƒå›¾åƒæ§åˆ¶çš„æ— åˆ†ç±»å™¨å¼•å¯¼å°ºåº¦ï¼ˆé»˜è®¤å€¼ï¼š1.0ï¼‰ã€‚
    *   `emo_cfg_scale`: (æµ®ç‚¹æ•°) æƒ…æ„Ÿæ§åˆ¶çš„æ— åˆ†ç±»å™¨å¼•å¯¼å°ºåº¦ï¼ˆé»˜è®¤å€¼ï¼š1.0ï¼‰ã€‚
    *   `model`: (ç»„åˆæ¡†) ä» `ComfyUI/models/Float/` ç›®å½•ä¸­é€‰æ‹©è¦ä½¿ç”¨çš„ FLOAT ä¸»æ¨¡å‹æ–‡ä»¶ (.pth)ã€‚
    *   `auto_crop`: (å¸ƒå°”å€¼) è‡ªåŠ¨è£å‰ªå‚è€ƒå›¾åƒä¸­çš„äººè„¸ã€‚ ï¼ˆé»˜è®¤ï¼šå…³é—­ï¼Œå¯èƒ½éœ€è¦æ—¶å¯ç”¨é»‘è‰²å¡«å……ï¼‰
*   **è¾“å‡º**:
    *   `animated_frames`: (å›¾åƒ) ç”Ÿæˆçš„åŠ¨ç”»å›¾åƒå¸§åºåˆ—ï¼ˆComfyUI æ ‡å‡† `IMAGE` æ ¼å¼ç”¨äºè§†é¢‘è¾“å‡ºï¼‰ã€‚
    *   `audio`: (éŸ³é¢‘) è¾“å…¥çš„éŸ³é¢‘å¯¹è±¡ã€‚
    *   `fps`: (æµ®ç‚¹æ•°) è¾“å‡ºåŠ¨ç”»çš„æ¯ç§’å¸§æ•°ã€‚
*   **ç”¨æ³•**: å°† `Load Image` èŠ‚ç‚¹è¿æ¥åˆ° `ref_image`ï¼Œå°† `Load Audio` èŠ‚ç‚¹è¿æ¥åˆ° `audio`ã€‚ä» `model` ä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©æ‚¨æƒ³è¦ä½¿ç”¨çš„ FLOAT æ¨¡å‹ã€‚æ ¹æ®éœ€è¦è°ƒæ•´ `fps`ã€å¼•å¯¼å°ºåº¦ï¼ˆ`aud_cfg_scale`ã€`ref_cfg_scale`ã€`emo_cfg_scale`ï¼‰ã€`emotion` å’Œ `auto_crop`ã€‚è®¾ç½® `seed` ä»¥ä¾¿ç»“æœå¯å¤ç°ã€‚è¾“å‡ºçš„ `animated_frames` å¯ä»¥è¿æ¥åˆ° `Save Image` èŠ‚ç‚¹ä»¥ä¿å­˜å›¾åƒåºåˆ—ï¼Œæˆ–è¿æ¥åˆ° `Video Combine` èŠ‚ç‚¹ï¼ˆä¾‹å¦‚ `VHS_VideoCombine`ï¼‰ä»¥åˆ›å»ºè§†é¢‘ã€‚è¾“å‡ºçš„ `audio` å¯ä»¥ä¼ é€’ç»™éŸ³é¢‘ç›¸å…³èŠ‚ç‚¹ï¼Œ`fps` è¾“å‡ºå¯ä»¥ç”¨äºé©±åŠ¨è§†é¢‘åˆæˆèŠ‚ç‚¹ã€‚

![image](https://github.com/KERRY-YUAN/ComfyUI_Float_Animator/blob/main/Examples/ComfyUI_Float_Animator.png)
![image](https://github.com/KERRY-YUAN/ComfyUI_Float_Animator/blob/main/Examples/Spark_TTS_&_Float_Animator.png)
---
---

## å®‰è£…æ­¥éª¤

1.  **å¯¼èˆªåˆ° ComfyUI `custom_nodes` ç›®å½•ï¼š**
    ```bash
    cd path/to/your/ComfyUI/custom_nodes
    ```
2.  **å…‹éš†æ­¤ä»“åº“ï¼š**
    ```bash
    git clone https://github.com/KERRY-YUAN/ComfyUI_Float_Animator.git ComfyUI_Float_Animator
    cd ComfyUI_Float_Animator
    ```
3.  **å®‰è£…ä¾èµ–é¡¹ï¼š**
    ä½¿ç”¨æ‚¨çš„ ComfyUI Python ç¯å¢ƒå®‰è£…æ‰€éœ€çš„ Python åº“ã€‚
    ```bash
    # Windows ä¸Š ComfyUI åµŒå…¥å¼ Python ç¤ºä¾‹:
    # path/to/your/ComfyUI/python_embeded/python.exe -m pip install -r requirements.txt
    # 
    # æˆ–è€…å¯¹äºç³»ç»Ÿçº§/è™šæ‹Ÿç¯å¢ƒ Python:
    pip install -r requirements.txt
    ```
    *æ³¨æ„ï¼šè¯·ç¡®ä¿ `torch` å’Œ `torchaudio` ç‰ˆæœ¬ä¸æ‚¨çš„ç³»ç»Ÿä»¥åŠ ComfyUI ç°æœ‰çš„ PyTorch å®‰è£…å…¼å®¹ã€‚*

## ğŸ“¥ æ¨¡å‹å’Œæ•°æ®è®¾ç½®

FLOAT æ¨¡å‹åŠå…¶ç›¸å…³æ•°æ®**å¿…é¡»æ”¾ç½®åœ¨ç‰¹å®šçš„é»˜è®¤ä½ç½®**ï¼ŒèŠ‚ç‚¹æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚æ­¤èŠ‚ç‚¹åŒ…ç°åœ¨æ”¯æŒ**åœ¨é¦–æ¬¡åŠ è½½èŠ‚ç‚¹æˆ–æ¨¡å‹ç¼ºå¤±æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹**ã€‚ ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨æ­¤èŠ‚ç‚¹åŒ…æä¾›çš„ Model_Download.bat è„šæœ¬é¢„å…ˆè‡ªåŠ¨ä¸‹è½½å¹¶æ”¾ç½®æ¨¡å‹ã€‚

1.  **æ¨¡å‹ä½ç½®ï¼š**
    `float.pth` ä¸»æ¨¡å‹ã€`wav2vec2-base-960h` æ–‡ä»¶å¤¹å’Œ `wav2vec-english-speech-emotion-recognition` æ–‡ä»¶å¤¹å¿…é¡»ä½äºï¼š`ComfyUI/models/Float/`
	`float.pth`ï¼šhttps://drive.google.com/file/d/1rvWuM12cyvNvBQNCLmG4Fr2L1rpjQBF0/view?usp=sharing
	`wav2vec2-base-960h`ï¼šhttps://huggingface.co/facebook/wav2vec2-base-960h
	`wav2vec-english-speech-emotion-recognition`ï¼šhttps://huggingface.co/r-f/wav2vec-english-speech-emotion-recognition

2.  **èŠ‚ç‚¹å†…è‡ªåŠ¨ä¸‹è½½ï¼š**
    åœ¨ ComfyUI ä¸­åŠ è½½å·¥ä½œæµ `Float_Animator` èŠ‚ç‚¹ã€‚èŠ‚ç‚¹ä¼šè‡ªåŠ¨æ£€æŸ¥å¹¶ä¸‹è½½æ‰€éœ€çš„æ¨¡å‹ã€‚è¯·è€å¿ƒç­‰å¾…ä¸‹è½½å®Œæˆï¼Œè¿™å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ã€‚ä¸‹è½½å®Œæˆåï¼Œæ‚¨å¯ä»¥é‡æ–°åŠ è½½é¡µé¢æˆ–åˆ·æ–°èŠ‚ç‚¹ä»¥ç¡®ä¿æ¨¡å‹è¢«æ­£ç¡®åŠ è½½ã€‚

3.  **æ‰‹åŠ¨ä¸‹è½½ï¼ˆå¯é€‰/æ•…éšœæ’é™¤ï¼‰ï¼š**
    å¦‚æœèŠ‚ç‚¹å†…çš„è‡ªåŠ¨ä¸‹è½½è¿‡ç¨‹å¤±è´¥ï¼Œæˆ–è€…æ‚¨å¸Œæœ›æå‰ä¸‹è½½æ‰€æœ‰æ¨¡å‹ï¼Œè¯·å¯¼èˆªåˆ° `ComfyUI_Float_Animator` ç›®å½•å¹¶è¿è¡Œæä¾›çš„æ‰¹å¤„ç†è„šæœ¬ï¼š
    ```bash
    cd ComfyUI/custom_nodes/ComfyUI_Float_Animator
    .\Model_Download.bat # åœ¨ Windows ä¸Šè¿è¡Œæ­¤è„šæœ¬
    ```
    ï¼ˆå¯¹äº Linux/macOS ç”¨æˆ·ï¼Œè¯·å‚è€ƒ `Model_Download.bat` çš„å†…å®¹æˆ–åŸå§‹ FLOAT ä»“åº“ä»¥è·å–æ‰‹åŠ¨ä¸‹è½½å‘½ä»¤ï¼Œæˆ–è€…ç›´æ¥è¿è¡Œ `python model_download/model_download.py`ï¼‰ã€‚

4.  **ç›®å½•ç»“æ„å‚è€ƒï¼š**
    å¿…éœ€çš„æœ€ç»ˆæ–‡ä»¶æ¶æ„å¦‚ä¸‹ï¼š

    ```
    ComfyUI/
    â”œâ”€â”€ custom_nodes/
    â”‚   â””â”€â”€ ComfyUI_Float_Animator/
    â”‚       â”œâ”€â”€ models/             # FLOAT çš„å†…éƒ¨æ¨¡å‹å®šä¹‰
    â”‚       â”‚   â”œâ”€â”€ float/          
    â”‚       â”‚   â”œâ”€â”€ wav2vec2.py
    â”‚       â”‚   â”œâ”€â”€ wav2vec2_ser.py
    â”‚       â”‚   â””â”€â”€ __init__.py
    â”‚       â”œâ”€â”€ model_download/     # æ¨¡å‹ä¸‹è½½è„šæœ¬å’Œé…ç½®æ–‡ä»¶
    â”‚       â”‚   â”œâ”€â”€ model_download.py
    â”‚       â”‚   â”œâ”€â”€ model_list.json # æ¨¡å‹åˆ—è¡¨ï¼ˆJSON æ ¼å¼ï¼‰
    â”‚       â”‚   â””â”€â”€ __init__.py
    â”‚       â”œâ”€â”€ options/
    â”‚       â”‚   â”œâ”€â”€ base_options.py
    â”‚       â”‚   â””â”€â”€ __init__.py
    â”‚       â”œâ”€â”€ Node.py
    â”‚       â”œâ”€â”€ Model_Download.bat  # æ‰‹åŠ¨ä¸‹è½½è„šæœ¬
    â”‚       â”œâ”€â”€ requirements.txt
    â”‚       â””â”€â”€ ... (å…¶ä»–åŒ…å†…æ–‡ä»¶)
    â””â”€â”€ models/
        â””â”€â”€ Float/
            â”œâ”€â”€ float.pth                           # ä¸» FLOAT æ¨¡å‹æ£€æŸ¥ç‚¹
            â”œâ”€â”€ wav2vec2-base-960h/                 # éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹æ–‡ä»¶å¤¹
            â”‚   â””â”€â”€ ... (Hugging Face æ–‡ä»¶)
            â””â”€â”€ wav2vec-english-speech-emotion-recognition/  # æƒ…æ„Ÿç¼–ç å™¨æ¨¡å‹æ–‡ä»¶å¤¹
                â””â”€â”€ ... (Hugging Face æ–‡ä»¶)
    ```

## ğŸ’¡ åˆæˆè´¨é‡ä¼˜åŒ–æç¤º

*   **æ­£é¢å¤´éƒ¨å§¿æ€**: FLOAT æ¨¡å‹åœ¨æ­£é¢å¤´éƒ¨å§¿æ€çš„å›¾åƒä¸Šè¡¨ç°æœ€ä½³ã€‚éæ­£é¢å›¾åƒå¯èƒ½ä¼šå¯¼è‡´æ•ˆæœä¸ä½³ã€‚
*   **äººè„¸è£å‰ª**: `auto_crop` é€‰é¡¹ç°åœ¨é»˜è®¤å¤„äº**å…³é—­**çŠ¶æ€ã€‚å¦‚æœæ‚¨çš„å‚è€ƒå›¾åƒæ˜¯å®Œæ•´çš„å¤´éƒ¨ç…§ç‰‡ï¼Œå¹¶ä¸”å¯èƒ½ä»è‡ªåŠ¨è£å‰ªå’Œç¼©æ”¾ä¸­å—ç›Šï¼ˆè¿™å¯èƒ½ä¼šåœ¨äººè„¸å‘¨å›´å¼•å…¥é»‘è‰²å¡«å……åŒºåŸŸï¼‰ï¼Œæ‚¨åº”è¯¥**å¯ç”¨**æ­¤é€‰é¡¹ã€‚åŸå§‹çš„ FLOAT é¡¹ç›®å»ºè®®è£å‰ªä»¥è·å¾—æœ€ä½³æ•ˆæœã€‚
*   **éŸ³é¢‘è´¨é‡**: ä¸ºäº†è·å¾—æœ€ä½³çš„å”‡å½¢åŒæ­¥å’Œæƒ…æ„Ÿç”Ÿæˆæ•ˆæœï¼Œè¯·ä½¿ç”¨å¹²å‡€çš„éŸ³é¢‘ï¼Œé¿å…èƒŒæ™¯éŸ³ä¹è¿‡é‡ã€‚åƒ [ClearVoice](https://github.com/modelscope/ClearerVoice-Studio) è¿™æ ·çš„å·¥å…·å¯ä»¥å¸®åŠ©æå–äººå£°ã€‚

## â— è®¸å¯è¯ â—

æ­¤ ComfyUI èŠ‚ç‚¹å°è£…å™¨çš„ä»£ç æ ¹æ® MIT è®¸å¯è¯å‘å¸ƒï¼ˆè¯¦è§æ­¤ä»“åº“ä¸­çš„ `LICENSE.md`ï¼‰ã€‚

**ç„¶è€Œï¼Œåº•å±‚çš„ FLOAT æ¨¡å‹åŠå…¶æ ¸å¿ƒæ¨ç†ä»£ç ï¼ˆåŒ…æ‹¬ `models/` å’Œ `options/` ç›®å½•ä¸­çš„æ–‡ä»¶ï¼Œä»¥åŠä»å®˜æ–¹ `generate.py` æ”¹ç¼–çš„éƒ¨åˆ†ï¼‰æ ¹æ® [çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§-ç¦æ­¢æ¼”ç» 4.0 å›½é™…å…¬å…±è®¸å¯è¯ (CC BY-NC-ND 4.0)](https://creativecommons.org/licenses/by-nc-nd/4.0/) å‘å¸ƒã€‚**

**è¿™æ„å‘³ç€åº•å±‚çš„ FLOAT æŠ€æœ¯ä¸èƒ½ç”¨äºå•†ä¸šç›®çš„ï¼Œå¹¶ä¸”ä¸å…è®¸è¿›è¡Œæ”¹ç¼–ï¼ˆé™¤äº†ä¸ºæ¡†æ¶å…¼å®¹æ€§è€Œè¿›è¡Œçš„å¿…è¦æŠ€æœ¯é›†æˆï¼‰ã€‚** è¯·æŸ¥é˜… `LICENSE.md` æ–‡ä»¶äº†è§£å®Œæ•´è¯¦æƒ…ã€‚

æœ‰å…³åŸå§‹ FLOAT é¡¹ç›®çš„å•†ä¸šå’¨è¯¢ï¼Œè¯·æŒ‰ç…§å…¶å®˜æ–¹ä»“åº“çš„è¯´æ˜è”ç³» `daniel@deepbrain.io`ã€‚

## ğŸ™ è‡´è°¢

æ­¤é¡¹ç›®æ˜¯ Taekyung Kiã€Dongchan Min å’Œ Gyeongsu ä¼˜ç§€å·¥ä½œçš„ ComfyUI å°è£…ï¼š
```bibtex
@article{ki2024float,
  title={FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait},
  author={Ki, Taekyung and Min, Dongchan and Chae, Gyeongsu},
  journal={arXiv preprint arXiv:2412.01064},
  year={2024}
}
```
æˆ‘ä»¬æ„Ÿè°¢åŸä½œè€…çš„å®è´µè´¡çŒ®å¹¶å¼€æºå…¶ä»£ç ã€‚æ„Ÿè°¢åŸå§‹ [FLOAT](https://github.com/deepbrainai-research/float) é¡¹ç›®çš„å¼€å‘è€…æä¾›äº†æ­¤èŠ‚ç‚¹ä¸­ä½¿ç”¨çš„å¼ºå¤§æ¨¡å‹å’Œåº“ç»„ä»¶ã€‚